{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TripletLoss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GbfihYmQEE3p"
      },
      "source": [
        "## Applying different strategy of triplet loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig3t89soACEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOCAL = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lAYsM42ZEE3r",
        "outputId": "4f49671e-7ace-46da-bce8-eb36b9a24424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%load_ext skip_cell"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bUBKCzFOEIuP",
        "outputId": "93556b68-2111-4bda-9666-2f6866c93354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "%%skip $LOCAL\n",
        "#Mounting the drive\n",
        "\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dMfXjni6EVvw",
        "colab": {}
      },
      "source": [
        "# %%skip $LOCAL\n",
        "\n",
        "!cp -a \"/content/drive/My Drive/triplets/\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CrXtpk6IBmaN"
      },
      "source": [
        "Setting up tensorboard for PyTorch in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "itWR2UmZfKh4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-PsFY4nOe8wH",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import random\n",
        "import time\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import SVHN\n",
        "from torchvision.models import resnet18\n",
        "from typing import Union\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u9XsQmkEe42o",
        "colab": {}
      },
      "source": [
        "from triplets.datasets import TripletSVHN\n",
        "from triplets.losses import TripletLoss, TripletSoftLoss, BatchAllTripletLoss, BatchHardTripletLoss, OnlineTripletLoss\n",
        "from triplets.metrics import mean_average_precision\n",
        "from triplets.nets import TripletNet\n",
        "from triplets.train import train\n",
        "from triplets.extractor import FeatureExtractor\n",
        "from triplets.samplers import BalancedBatchSampler\n",
        "from triplets.selectors import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, \\\n",
        "                               SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
        "from triplets.utils import freeze_layers\n",
        "from triplets.visualisation import plot_grad_flow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mWFBcpDTMgH0",
        "colab": {}
      },
      "source": [
        "n_features = 512\n",
        "n_classes = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 100\n",
        "validation_split = 0.2\n",
        "shuffle_dataset = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YXzgWgceewZt",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uqoPphSxeVAW",
        "outputId": "3e5d95e4-c00d-4217-cdc8-6b1dded17198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset = SVHN(root = 'data/', download=True, split='train')\n",
        "train_size = int(0.8*len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "dataset_train, dataset_valid = random_split(dataset, [train_size, valid_size])\n",
        "dataset_test = SVHN(root = 'data/', download=True, split='test');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: data/train_32x32.mat\n",
            "Using downloaded and verified file: data/test_32x32.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tmvv8Y1kJWRs"
      },
      "source": [
        "# Setting parameters for training model with triplet loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mMK3PXazxTeD",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "num_triplets = 1\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-iRwgCIpMofC",
        "colab": {}
      },
      "source": [
        "model_base = resnet18(pretrained=True)\n",
        "model_base.eval();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6B_d4jRlMw3a"
      },
      "source": [
        "Defining extractor using custom class to extract features from last cnn pretrained layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qguF2NEzM0JR",
        "colab": {}
      },
      "source": [
        "extractor = FeatureExtractor(model=model_base, n_remove_layers=1, n_features=n_features, device=device)\n",
        "extracted_resnet = extractor.prepare_model()\n",
        "#Freezing all but two last layers\n",
        "extracted_resnet = freeze_layers(extracted_resnet, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P7IH2ElAwxFu",
        "colab": {}
      },
      "source": [
        "preprocess = transforms.Compose([            \n",
        " transforms.Resize(256),                    \n",
        " transforms.CenterCrop(224),                \n",
        " transforms.ToTensor(),                     \n",
        " transforms.Normalize(                      \n",
        " mean=[0.485, 0.456, 0.406],                \n",
        " std=[0.229, 0.224, 0.225]                  \n",
        " )])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCL8EUgIACE_",
        "colab_type": "text"
      },
      "source": [
        "## Training with softmax triplet loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DkVNpdpewhe",
        "colab_type": "text"
      },
      "source": [
        "Taken fron the paper titled \"DEEP METRIC LEARNING USING TRIPLET NETWORK\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "https://arxiv.org/pdf/1412.6622.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0QSFZ0PFxNQF",
        "colab": {}
      },
      "source": [
        "triplet_train= TripletSVHN(dataset, dataset_train.indices, dataset_valid.indices,\n",
        "                            preprocess, 'train', SEED)\n",
        "triplet_valid = TripletSVHN(dataset, dataset_train.indices, dataset_valid.indices,\n",
        "                            preprocess, 'val', SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-7BUNKDxRDI",
        "colab": {}
      },
      "source": [
        "dataloader_train = torch.utils.data.DataLoader(triplet_train, batch_size=batch_size)\n",
        "dataloader_valid = torch.utils.data.DataLoader(triplet_valid, batch_size=batch_size)\n",
        "dataloaders = {'train': dataloader_train, 'val': dataloader_valid}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LUO5nexkKJiN",
        "colab": {}
      },
      "source": [
        "model = TripletNet(extracted_resnet)\n",
        "criterion = TripletSoftLoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JZb-S7sBhXuO",
        "colab": {}
      },
      "source": [
        "train(model, dataloaders, criterion, optimizer, scheduler, epochs, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHZ73p_ZACFU",
        "colab_type": "text"
      },
      "source": [
        "## Training with triplet loss provided FaceNet paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1mjIkCliVRj",
        "colab_type": "text"
      },
      "source": [
        "We are going to use custom triplet dataset, which provides possibility of testing model on fixed sample and creates valid triplets in every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ACNU2gbACFV",
        "colab": {}
      },
      "source": [
        "triplet_train= TripletSVHN(dataset, dataset_train.indices, dataset_valid.indices,\n",
        "                            preprocess, 'train', SEED)\n",
        "triplet_valid = TripletSVHN(dataset, dataset_train.indices, dataset_valid.indices,\n",
        "                            preprocess, 'val', SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4qfM-wpPACFY",
        "colab": {}
      },
      "source": [
        "dataloader_train = torch.utils.data.DataLoader(triplet_train, batch_size=batch_size)\n",
        "dataloader_valid = torch.utils.data.DataLoader(triplet_valid, batch_size=batch_size)\n",
        "dataloaders = {'train': dataloader_train, 'val': dataloader_valid}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Loh85dBJACFb",
        "colab": {}
      },
      "source": [
        "model = TripletNet(extracted_resnet)\n",
        "criterion = TripletLoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qz0yUzOLACFj",
        "colab": {}
      },
      "source": [
        "train(model, dataloaders, criterion, optimizer, scheduler, epochs, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZKu96HACFm",
        "colab_type": "text"
      },
      "source": [
        "## Adding hard mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmiWphFJ-MUa",
        "colab_type": "text"
      },
      "source": [
        "In order to improve possible triplet selection we are going to use online triplet mining. It allows us to compute possible triplets in every iteration of the dataset, constructing $B^3$ triplets out of computed $B$ **embeddings**. Most of them are not relevant and we are going to use two strategies from which one can select possible triplets. In order to improve possible triplet selection we are going to use online triplet mining. Suppose that you have a batch of faces as input of size $B = P K$ , composed of $P$ different persons with $K$ images each. A typical value is $K = 4$ . The two strategies are:\n",
        "\n",
        "**batch all**: select all the valid triplets, and average the loss on the hard and semi-hard triplets. a crucial point here is to not take into account the easy triplets (those with loss 0 ), as averaging on them would make the overall loss very small this produces a total of $P K ( K − 1 ) ( P K − K )$ triplets $P K$ anchors, $K − 1$ possible positives per anchor, $P K − K$ possible negatives) \n",
        "<br>\n",
        "**batch hard**: for each anchor, select the hardest positive (biggest distance $d ( a , p ) )$ and the hardest negative among the batch this produces $P K$ triplets the selected triplets are the hardest among the batch.\n",
        "<br>\n",
        "[1] https://omoindrot.github.io/triplet-loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xFXU0sHACFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = FeatureExtractor(model=model_base, n_remove_layers=1, n_features=n_features, device=device)\n",
        "extracted_resnet = extractor.prepare_model()\n",
        "extracted_resnet = freeze_layers(extracted_resnet, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtykUjG2ACFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_sampler = BalancedBatchSampler(dataset.labels[dataset_train.indices], n_classes=10, n_samples=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7DORLZiACF0",
        "colab_type": "code",
        "outputId": "2d0f38b7-3dde-44cf-c6a1-90b4fa1ff05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset = SVHN(root = 'data/', download=True, split='train', transform=preprocess)\n",
        "train_size = int(0.8*len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "dataset_train, dataset_valid = random_split(dataset, [train_size, valid_size])\n",
        "dataset_test = SVHN(root = 'data/', download=True, split='test', transform=preprocess);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: data/train_32x32.mat\n",
            "Using downloaded and verified file: data/test_32x32.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LYTg-YsACF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
        "train_batch_sampler = BalancedBatchSampler(dataset.labels[dataset_train.indices], n_classes=10, n_samples=25)\n",
        "valid_batch_sampler = BalancedBatchSampler(dataset.labels[dataset_valid.indices], n_classes=10, n_samples=25)\n",
        "\n",
        "online_train_loader = torch.utils.data.DataLoader(dataset_train, batch_sampler=train_batch_sampler)\n",
        "online_valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_sampler=valid_batch_sampler)\n",
        "\n",
        "margin = 1.\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(extracted_resnet.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "n_epochs = 20\n",
        "log_interval = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8kHajx3HACF6",
        "colab": {}
      },
      "source": [
        "dataloaders = {'train': online_train_loader, 'val': online_valid_loader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MhGIGk5PACF9",
        "colab": {}
      },
      "source": [
        "criterion = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(extracted_resnet.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V40LbjyORoHx",
        "colab_type": "text"
      },
      "source": [
        "# Optimizing the margin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXutqnZtRrP6",
        "colab_type": "text"
      },
      "source": [
        "I dedcided to use the loss function with online triplet mining technique in order to converge faster to desirable solution. \n",
        "\n",
        "* The first step was to calculate loss on both training and validation set, choosing model with lowest validation loss. \n",
        "* After that the kNN model was trained on the features obtained from the training with hyperparameter (number of neighbors) found in previous optimilisation (done directly on CNN codes)\n",
        "* One can argue that this strategy is not optimal - we probably should tune both margin and number of neighors at the same time (for example using RandomSearch or BayesianOptimisation) however due to lack of training resourches we are going to stay with this grid search strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG1KJvK9gub1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=20\n",
        "margins = [0.01, 0.005, 0.1, 0.5, 1, 5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2eWJpI65ACGG",
        "colab": {}
      },
      "source": [
        "for margin in margins:\n",
        "    criterion = OnlineTripletLoss(margin, AllTripletSelector())\n",
        "    semihard_trained_net, semihard_total_loss_train, semihard_total_loss_val = \\\n",
        "        train(extracted_resnet, dataloaders, criterion, optimizer, scheduler, epochs, \n",
        "        device, model_name='hardmine_all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4IxDe7DAQCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = FeatureExtractor(model=semihard_trained_net, n_remove_layers=0, n_features=n_features, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPqdb_4haQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=25)\n",
        "features = extractor.extract_features(dataset_train)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}